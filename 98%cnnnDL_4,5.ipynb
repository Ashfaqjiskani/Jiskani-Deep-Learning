{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HandsonDL#4,5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uaue4e5U4PQO7pD_6SLZdHV6TLo__o70",
      "authorship_tag": "ABX9TyPND3WiOcbwJPoWqROW5eT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashfaqjiskani/jiskani/blob/master/98%25cnnnDL_4%2C5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghuPJKM2fxJG",
        "outputId": "a1e14885-2a43-4b91-e7a7-22902585227f"
      },
      "source": [
        "#Hands on Deep Learning Topic 4 How to code a single neuron? \r\n",
        "inputs = [4, 7, 8]\r\n",
        "weights = [0.4, 0.6, 0.8]\r\n",
        "biase = 12\r\n",
        "output = (inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+biase)\r\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24.200000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFUfV0m4gjz5",
        "outputId": "fe5fc3e3-1031-4e08-d0a9-f48a608be8a0"
      },
      "source": [
        "#Hands on Deep Learning Topic 4 How to code a single neuron? \r\n",
        "inputs = [4, 7, 8]\r\n",
        "weights = [-0.4, -0.6, -0.8]\r\n",
        "biase = 12\r\n",
        "output = (inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+biase)\r\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.20000000000000107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHQFpKLZg3JZ",
        "outputId": "9f739dc6-a51e-4d2b-d09d-4d8fb768010f"
      },
      "source": [
        "#Hands on Deep Learning Topic 4 How to code a single neuron? \r\n",
        "inputs = [4, 7, 8]\r\n",
        "weights = [-0.4, -0.6, -0.8]\r\n",
        "biase = 0\r\n",
        "output = (inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+biase)\r\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-12.200000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SniT0MMzg63U",
        "outputId": "ae7c2750-fe17-49d8-90bc-ff8d16ee72a6"
      },
      "source": [
        "#Hands on Deep Learning Topic 4 How to code a single neuron? \r\n",
        "inputs = [4, 7, 8]\r\n",
        "weights = [-0.4, -0.6, -0.8]\r\n",
        "biase = -12\r\n",
        "output = (inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+biase)\r\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-24.200000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSVYa1RHjnY6",
        "outputId": "54a65d1f-4b77-4d13-8694-cde5b918eb44"
      },
      "source": [
        "#Deep Learning Fundamentals, Topic # 5 How to code a Multi neuron with 3neurons\r\n",
        "inputs = [6, 7, -9, 8]\r\n",
        "weights1 = [-0.8, 9, 1,7]\r\n",
        "weights2 = [1, 3, 5, 6]\r\n",
        "weights3 = [4, 6, 8, 8]\r\n",
        "biase1 = 2\r\n",
        "biase2 = 4\r\n",
        "biase3 = 5\r\n",
        "outputs  = [\r\n",
        "        #Neuron1\r\n",
        "         inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3]+biase1,\r\n",
        "        #Neuron2\r\n",
        "         inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3]+biase2,\r\n",
        "        #Neuron3\r\n",
        "        inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+inputs[3]*weights3[3]+biase3]\r\n",
        "print(outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[107.2, 34, 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H0qP7P0mjQp",
        "outputId": "50370858-73bc-4bfa-ff11-5ddc0d48fe29"
      },
      "source": [
        "# Topic # 5 How to code a Multi neuron with 4 neurons\r\n",
        "inputs = [6, 7, -9, 8, 9]\r\n",
        "weights1 = [-0.8, 9, 1,7,8]\r\n",
        "weights2 = [1, 3, 5, 6, 5]\r\n",
        "weights3 = [4, 6, 8, 8, 5]\r\n",
        "weights4 = [-2, -5, 7, 8, 6]\r\n",
        "biase1 = 2\r\n",
        "biase2 = 4\r\n",
        "biase3 = 5\r\n",
        "biase4= 6\r\n",
        "outputs  = [\r\n",
        "        #Neuron1\r\n",
        "         inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3]+inputs[4]*weights1[4]+biase1,\r\n",
        "        #Neuron2\r\n",
        "         inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3]+inputs[4]*weights2[4]+biase2,\r\n",
        "        #Neuron3\r\n",
        "        inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+inputs[3]*weights3[3]+inputs[4]*weights3[4]+biase3,\r\n",
        "         #Neuron4\r\n",
        "        inputs[0]*weights4[0]+inputs[1]*weights4[1]+inputs[2]*weights4[2]+inputs[3]*weights4[3]+inputs[4]*weights4[4]+biase4]\r\n",
        "        \r\n",
        "print(outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[179.2, 79, 108, 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL-Jfar6oUOR",
        "outputId": "6c31e420-f853-4ee8-ba70-b684f60a26bd"
      },
      "source": [
        "# # 6 Multi Neuron Code Loop \r\n",
        "inputs= [1,2, 3, 4]\r\n",
        "weights = [[0.2,0.8,-0.5],[0.5, -0.91, 0.26],[-0.26,-0.27, 0.17]]\r\n",
        "biases = [3, 4, 6]\r\n",
        "#ouput of the current layer\r\n",
        "layer_outputs = []\r\n",
        "#for each neuron\r\n",
        "for neuron_weights, neuron_bias in zip(weights, biases):\r\n",
        "  #zerod output of given neuron\r\n",
        "  neuron_output = 0\r\n",
        "  for n_input, weight in zip(inputs, neuron_weights):\r\n",
        "#multipily this with assocaited weights # and add the neuron output variable\r\n",
        "    neuron_output += n_input*weight\r\n",
        "#Addd biase\r\n",
        "neuron_output += neuron_bias\r\n",
        "#put neuron result to the layer output list\r\n",
        "layer_outputs.append(neuron_output)\r\n",
        "#print(layer_outputs)\r\n",
        "print(neuron_weights)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.26, -0.27, 0.17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo5C92fctvNL",
        "outputId": "b6dd5f5f-52e1-4020-b078-2cad9f92306f"
      },
      "source": [
        "#Deep Learning Fundamentals, Topic # 6 Multi Neuron Code Loop \r\n",
        "inputs= [1,2, 3, 4]\r\n",
        "weights = [[0.2,0.8,-0.5],[0.5, -0.91, 0.26],[-0.26,-0.27, 0.17]]\r\n",
        "biases = [3, 4, 6]\r\n",
        "#ouput of the current layer\r\n",
        "layer_outputs = []\r\n",
        "#for each neuron\r\n",
        "for neuron_weights, neuron_bias in zip(weights, biases):\r\n",
        "  #zerod output of given neuron\r\n",
        "  neuron_output = 0\r\n",
        "  for n_input, weight in zip(inputs, neuron_weights):\r\n",
        "#multipily this with assocaited weights # and add the neuron output variable\r\n",
        "    neuron_output += n_input*weight\r\n",
        "#Addd biase\r\n",
        "neuron_output += neuron_bias\r\n",
        "#put neuron result to the layer output list\r\n",
        "layer_outputs.append(neuron_output)\r\n",
        "print(layer_outputs)\r\n",
        "print(neuron_weights)\r\n",
        "print(neuron_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.71]\n",
            "[-0.26, -0.27, 0.17]\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJpC8XHPuBay",
        "outputId": "3d410ad0-3c2f-4f0d-d208-c0748c86acf8"
      },
      "source": [
        "#Deep Learning Fundamentals, Topic # 7 Coding Multi Neuron with Numpy\r\n",
        "# in python there are many libraraiers for mathematical opertion we used NUmpy libaraiers in python\r\n",
        "inputs = [2, 8, 4]\r\n",
        "weights = [0.2, 0.7, 0.8]\r\n",
        "bias = 3.0\r\n",
        "#y=x.w+b\r\n",
        "import numpy as np\r\n",
        "output = np.dot(weights, inputs)+bias\r\n",
        "print(output)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cA5NtawwKYi",
        "outputId": "9d33571d-47d1-4af6-8caa-1a63ad368d6a"
      },
      "source": [
        "#Deep Learning Fundamentals, Topic # 7 Coding Multi Neuron with Numpy\r\n",
        "#  for Multple neurons\r\n",
        "inputs= [1,2, 3, 4]\r\n",
        "weights = [[0.2,0.8,-0.5, 0.4],[0.5, -0.91, 0.26, 0.6],[-0.26,-0.27, 0.17, 0.7]]\r\n",
        "biases = [3, 4, 6]\r\n",
        "#y=x.w+b\r\n",
        "import numpy as np\r\n",
        "layer_outputs = np.dot(weights, inputs)+biases\r\n",
        "print(layer_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.9  5.86 8.51]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NaKmXuDxCIP",
        "outputId": "343078c2-460f-4af4-9832-d49ad5bed97e"
      },
      "source": [
        "#Topic # 7 Coding Multi Neuron with Numpy\r\n",
        "#  for Multple neurons \r\n",
        "inputs= [1,2, 3, 4]\r\n",
        "weights = [[0.2,0.8,-0.5, 0.4],[0.5, -0.91, 0.26, 0.6],[-0.26,-0.27, 0.17, 0.7]]\r\n",
        "biases = [13, 14, 16]\r\n",
        "#y=x.w+b\r\n",
        "import numpy as np\r\n",
        "layer_outputs = np.dot(weights, inputs)+biases\r\n",
        "print(layer_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14.9  15.86 18.51]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPIVjBEuxlQ3",
        "outputId": "9935dcfb-b787-452d-8381-948beea64745"
      },
      "source": [
        "# # 8 Batch Data in NumPy \r\n",
        "#batach is helpful for easy computaion and make it simple\r\n",
        "#Topic # 9 Adding more Hidden Layers (Urdu|Hindi)\r\n",
        "import numpy as np\r\n",
        "inputs= [1,2, 3, 4]\r\n",
        "weights = [[0.6,0.9,-0.5, 0.1],\r\n",
        "           [0.4, -0.91, 0.26, -0.5],\r\n",
        "           [-0.26,-0.7, 0.17, 0.87]]\r\n",
        "biases = [1, 4, 6]\r\n",
        "weights2 = [[0.1,-0.13,0.5],\r\n",
        "           [-0.6, 0.12, -0.33],\r\n",
        "           [-0.44, 0.93, -0.12]]\r\n",
        "biases2 = [2, 5, 7]\r\n",
        "output1 = np.dot(inputs, np.array(weights).T)+biases2\r\n",
        "output2 = np.dot(output1, np.array(weights2).T)+biases2\r\n",
        "print(output1)\r\n",
        "print(output2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.3  2.36 9.33]\n",
            "[6.6882 0.2243 6.6232]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEVIoyStnP4-"
      },
      "source": [
        "#Topic # 10 Activation Function \r\n",
        "#ap nain aksar daikha hoga, k agr ap ghar main bethhy hain and ghar main khana bna raha ho ap ko smell atii to ap kitichen ke traf attract hoty hain \r\n",
        "#same agr ap gram chez ko touch kariean to ahath ko pocahay hatiean gay basically ap k damagh main signal jata hy \r\n",
        "# Rotton egg \r\n",
        "# same exmale of gate open if u are owner while not open when outsider it mean inpuat has some specfic value this process is active via activation function\r\n",
        "#input minus infinty to infity how neuron decide to fire\r\n",
        "#to decide the neuron is fire via activation function 0---1, -1...1\r\n",
        "x=2\r\n",
        "f(x)=2x=4\r\n",
        "#some data is not linear its not fit in straight line some times nonlinear data so activation function help to deal n on linear data\r\n",
        "#output pass through activation finction and convertr it into 0 and 1, 1;;;-1 and predicat output how much equal to input whither neuron is fire or not or responsd or not\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqcw1ZULrMw7"
      },
      "source": [
        "## 11 Activation Function Types \r\n",
        "#Linear Activation Function.. linear relationship between input and output and fit via straight line data should linear for linaer activation function\r\n",
        "# sTEP Function... as y=wx+b x>0 y=1, x<0, y=0\r\n",
        "#RELU(Rectified linear activation function) y=wx+b as wx+b>0 y=x, if wx+b<0, y=0 it is most popular activation function\r\n",
        "#output= wX+b \r\n",
        "#softmax activation function "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ESZPsKHwFqd"
      },
      "source": [
        " # 12 Loss in Neural Network \r\n",
        "# students marks obtain in test and train it to reduce loss Target- actual=loss lets output=wx+b is 0.8, actual =1  actual-output>>1-0.8=0.2 loss= actual-prediction\r\n",
        "#Types of loss \r\n",
        "#data is continious regression, time depndent data \r\n",
        "# MSE mean square error  \r\n",
        "#mean absolute error\r\n",
        "#mean precntage or log error\r\n",
        "#When data is not continious its cataegotrical or discerete data binary or yes no form it mean output in the form of yes no\r\n",
        "#Binary yes/no \r\n",
        "#Non binary high/low/ medium yes/np balac/wht\r\n",
        "#when data is binary loss will be bionary cross entropy  so we measure probabailty calaculate in this error\r\n",
        "#Non binary categorical \r\n",
        "# yes/no/maybe black/wht/red low/high/medium\r\n",
        "# categogical cross entropy loss is used in non binary cataegorical class\r\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZsud7mr0gFb"
      },
      "source": [
        "#Topic # 13 Optimization\r\n",
        "# input>>NN>>output>>activation function>>prediction>>loss>>loss minimize\r\n",
        "#optomization is process to minimze the loss with specfic optimaztion paramertrs  there are various ways to done optomization \r\n",
        "#in machnie learaning lets see linaer regresion varuius data points so slope and intercept is change in such way styraight line touches max points as y=mx+c\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBbYX00t5zdE"
      },
      "source": [
        " # 14 Optimizers \r\n",
        "  \r\n",
        "# slope is vhange in y.change in x, slope away from point is postive(up the hill) , twaod point is negrtuve (downthe hill), we choose weight and biase such that losss is zero mean\r\n",
        "# mean actual and predicated value are equal or same, loss deivuide/weigts and chk the loss is increased or decreased \r\n",
        "# new weight wn= wd old weigt -eightaa(learning rate)(partaialloss/oldwt)\r\n",
        "#lets oldwt=2 loss=4 eitaa=0.1  wn=2-0.1*4/2=1.8 this process is classed gradient descent (slope twoard down) it is optimzer GDc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhwn6lfVLqId"
      },
      "source": [
        "# 15 Types of Optimizers\r\n",
        "#Grdeint Descent to minimize loss to optomize the wt and biaze use back prpogation in this process input data once, it need alot computation power it takes alot of time\r\n",
        "# stotatechastc gradient descent  one by one data points and set optimze wt and baise this process is stataocahstic gradient\r\n",
        "#mini batch gradient descent        in this process data is given in mini bataches this best as comapred to GDC AND SGDC and faster less compuataion time and power is requed "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUHYOvrFO15J"
      },
      "source": [
        "# 16 What is EPOC & Forward Propagation \r\n",
        "# give input select wt randomly used biase and apply activation function give ouptput it is called Fwd propgation\r\n",
        "# But when we mimize thhe loss and from end to satart to adjaust wt and baise it is called Back propgation\r\n",
        "#onece fwd and once once back it is called one Epoc\r\n",
        "#iTRAION IN ONE EPOCHS HOW MANY ITERATION\r\n",
        "#LEST TOTAL DATA=10,000\r\n",
        "#MINI BATCH=1000\r\n",
        "=10,000/1000=10 IN ONE EPOCHS 10 ITRATION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ9pW5o0RdGh"
      },
      "source": [
        "# 17 Final Overview\r\n",
        "#HOW MANY INPUTS\r\n",
        "#HOW MANY OUPUTS\r\n",
        "#HOW MANY # HL\r\n",
        "#HOW MANY NEURONS IN EACH HL\r\n",
        "#ACTIVATION FUNCTION?\r\n",
        "#OPTOMIZER?\r\n",
        "#EPOCHS?\r\n",
        "#BATCH SIZE?\r\n",
        "#LEARNING RATE?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U3muKg-UhQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cfb7a8a-6faa-4200-cd21-1317b9e320bb"
      },
      "source": [
        "#Deep Learning Hands- On Topic # 1 Google Colab Mounting \r\n",
        "#Topic # 17 Keras- Image Classification (Urdu | Hindi)\r\n",
        "#import libabaries\r\n",
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "#import kears libraraies\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import  Dense\r\n",
        "from tensorflow.keras.layers import Flatten\r\n",
        "#Load MNIST dATA\r\n",
        "mnist = tf.keras.datasets.mnist\r\n",
        "(train_features,train_labels),(test_features,test_labels) = \\\r\n",
        "mnist.load_data()\r\n",
        "#normalize the data\r\n",
        "train_features, test_features = train_features/255.0, test_features / 255.0\r\n",
        "#build a sequentail model\r\n",
        "model = Sequential()\r\n",
        "model.add(Flatten(input_shape=(28,28)))\r\n",
        "model.add(Dense(units=500, activation='relu'))\r\n",
        "model.add(Dense(units=200, activation='relu'))\r\n",
        "model.add(Dense(units=100, activation='softmax'))\r\n",
        "#Compile the Model\r\n",
        "model.compile(optimizer = 'adam' , \\\r\n",
        "              loss = 'sparse_categorical_crossentropy' , \\\r\n",
        "              metrics = ['accuracy'])\r\n",
        "model.summary()\r\n",
        "#Train the Model\r\n",
        "model.fit(train_features,train_labels, epochs=100)\r\n",
        "model.evaluate(test_features,test_labels)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5000)              3925000   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2000)              10002000  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              2001000   \n",
            "=================================================================\n",
            "Total params: 15,928,000\n",
            "Trainable params: 15,928,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 228s 121ms/step - loss: 0.3461 - accuracy: 0.9022\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0900 - accuracy: 0.9724\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0619 - accuracy: 0.9816\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0476 - accuracy: 0.9853\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0374 - accuracy: 0.9888\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0353 - accuracy: 0.9897\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 226s 120ms/step - loss: 0.0338 - accuracy: 0.9904\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0250 - accuracy: 0.9930\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0211 - accuracy: 0.9946\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0271 - accuracy: 0.9933\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 226s 120ms/step - loss: 0.0288 - accuracy: 0.9931\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0176 - accuracy: 0.9948\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0197 - accuracy: 0.9955\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 226s 120ms/step - loss: 0.0178 - accuracy: 0.9955\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0124 - accuracy: 0.9966\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0174 - accuracy: 0.9960\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0165 - accuracy: 0.9963\n",
            "Epoch 18/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0195 - accuracy: 0.9962\n",
            "Epoch 19/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0132 - accuracy: 0.9969\n",
            "Epoch 20/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0225 - accuracy: 0.9960\n",
            "Epoch 21/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0116 - accuracy: 0.9975\n",
            "Epoch 22/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0141 - accuracy: 0.9973\n",
            "Epoch 23/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0260 - accuracy: 0.9956\n",
            "Epoch 24/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0155 - accuracy: 0.9973\n",
            "Epoch 25/100\n",
            "1875/1875 [==============================] - 228s 121ms/step - loss: 0.0168 - accuracy: 0.9976\n",
            "Epoch 26/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0228 - accuracy: 0.9958\n",
            "Epoch 27/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0130 - accuracy: 0.9978\n",
            "Epoch 28/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0143 - accuracy: 0.9972\n",
            "Epoch 29/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0158 - accuracy: 0.9976\n",
            "Epoch 30/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0129 - accuracy: 0.9980\n",
            "Epoch 31/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0150 - accuracy: 0.9975\n",
            "Epoch 32/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0175 - accuracy: 0.9977\n",
            "Epoch 33/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0101 - accuracy: 0.9982\n",
            "Epoch 34/100\n",
            "1875/1875 [==============================] - 228s 122ms/step - loss: 0.0218 - accuracy: 0.9970\n",
            "Epoch 35/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0101 - accuracy: 0.9980\n",
            "Epoch 36/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0166 - accuracy: 0.9978\n",
            "Epoch 37/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0106 - accuracy: 0.9984\n",
            "Epoch 38/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0230 - accuracy: 0.9970\n",
            "Epoch 39/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0177 - accuracy: 0.9983\n",
            "Epoch 40/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0104 - accuracy: 0.9978\n",
            "Epoch 41/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0243 - accuracy: 0.9982\n",
            "Epoch 42/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0166 - accuracy: 0.9976\n",
            "Epoch 43/100\n",
            "1875/1875 [==============================] - 228s 121ms/step - loss: 0.0080 - accuracy: 0.9989\n",
            "Epoch 44/100\n",
            "1875/1875 [==============================] - 228s 121ms/step - loss: 0.0081 - accuracy: 0.9989\n",
            "Epoch 45/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0212 - accuracy: 0.9976\n",
            "Epoch 46/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0111 - accuracy: 0.9987\n",
            "Epoch 47/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0048 - accuracy: 0.9993\n",
            "Epoch 48/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0174 - accuracy: 0.9980\n",
            "Epoch 49/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0118 - accuracy: 0.9986\n",
            "Epoch 50/100\n",
            "1875/1875 [==============================] - 228s 122ms/step - loss: 0.0184 - accuracy: 0.9983\n",
            "Epoch 51/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0129 - accuracy: 0.9980\n",
            "Epoch 52/100\n",
            "1875/1875 [==============================] - 228s 121ms/step - loss: 0.0120 - accuracy: 0.9986\n",
            "Epoch 53/100\n",
            "1875/1875 [==============================] - 228s 122ms/step - loss: 0.0106 - accuracy: 0.9988\n",
            "Epoch 54/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0174 - accuracy: 0.9982\n",
            "Epoch 55/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0118 - accuracy: 0.9988\n",
            "Epoch 56/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0193 - accuracy: 0.9977\n",
            "Epoch 57/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0118 - accuracy: 0.9993\n",
            "Epoch 58/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0083 - accuracy: 0.9991\n",
            "Epoch 59/100\n",
            "1875/1875 [==============================] - 226s 121ms/step - loss: 0.0167 - accuracy: 0.9984\n",
            "Epoch 60/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0118 - accuracy: 0.9985\n",
            "Epoch 61/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0144 - accuracy: 0.9984\n",
            "Epoch 62/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0201 - accuracy: 0.9987\n",
            "Epoch 63/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0099 - accuracy: 0.9987\n",
            "Epoch 64/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0071 - accuracy: 0.9990\n",
            "Epoch 65/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0351 - accuracy: 0.9981\n",
            "Epoch 66/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0077 - accuracy: 0.9991\n",
            "Epoch 67/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0052 - accuracy: 0.9992\n",
            "Epoch 68/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0076 - accuracy: 0.9992\n",
            "Epoch 69/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0196 - accuracy: 0.9985\n",
            "Epoch 70/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0142 - accuracy: 0.9990\n",
            "Epoch 71/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0093 - accuracy: 0.9993\n",
            "Epoch 72/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0136 - accuracy: 0.9990\n",
            "Epoch 73/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0204 - accuracy: 0.9988\n",
            "Epoch 74/100\n",
            "1875/1875 [==============================] - 228s 121ms/step - loss: 0.0169 - accuracy: 0.9986\n",
            "Epoch 75/100\n",
            "1875/1875 [==============================] - 230s 122ms/step - loss: 0.0079 - accuracy: 0.9993\n",
            "Epoch 76/100\n",
            "1875/1875 [==============================] - 230s 123ms/step - loss: 0.0154 - accuracy: 0.9985\n",
            "Epoch 77/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0045 - accuracy: 0.9993\n",
            "Epoch 78/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0060 - accuracy: 0.9993\n",
            "Epoch 79/100\n",
            "1875/1875 [==============================] - 230s 122ms/step - loss: 0.0129 - accuracy: 0.9987\n",
            "Epoch 80/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0253 - accuracy: 0.9982\n",
            "Epoch 81/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0133 - accuracy: 0.9991\n",
            "Epoch 82/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0183 - accuracy: 0.9988\n",
            "Epoch 83/100\n",
            "1875/1875 [==============================] - 230s 122ms/step - loss: 0.0051 - accuracy: 0.9994\n",
            "Epoch 84/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0196 - accuracy: 0.9987\n",
            "Epoch 85/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0097 - accuracy: 0.9992\n",
            "Epoch 86/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0066 - accuracy: 0.9991\n",
            "Epoch 87/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0164 - accuracy: 0.9989\n",
            "Epoch 88/100\n",
            "1875/1875 [==============================] - 227s 121ms/step - loss: 0.0103 - accuracy: 0.9990\n",
            "Epoch 89/100\n",
            "1875/1875 [==============================] - 228s 121ms/step - loss: 0.0113 - accuracy: 0.9990\n",
            "Epoch 90/100\n",
            "1875/1875 [==============================] - 228s 122ms/step - loss: 0.0175 - accuracy: 0.9986\n",
            "Epoch 91/100\n",
            "1875/1875 [==============================] - 228s 122ms/step - loss: 0.0149 - accuracy: 0.9994\n",
            "Epoch 92/100\n",
            "1875/1875 [==============================] - 231s 123ms/step - loss: 0.0366 - accuracy: 0.9980\n",
            "Epoch 93/100\n",
            "1875/1875 [==============================] - 230s 123ms/step - loss: 0.0101 - accuracy: 0.9991\n",
            "Epoch 94/100\n",
            "1875/1875 [==============================] - 230s 123ms/step - loss: 0.0130 - accuracy: 0.9991\n",
            "Epoch 95/100\n",
            "1875/1875 [==============================] - 230s 123ms/step - loss: 0.0085 - accuracy: 0.9992\n",
            "Epoch 96/100\n",
            "1875/1875 [==============================] - 230s 122ms/step - loss: 0.0162 - accuracy: 0.9990\n",
            "Epoch 97/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0116 - accuracy: 0.9991\n",
            "Epoch 98/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0156 - accuracy: 0.9992\n",
            "Epoch 99/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0202 - accuracy: 0.9991\n",
            "Epoch 100/100\n",
            "1875/1875 [==============================] - 229s 122ms/step - loss: 0.0188 - accuracy: 0.9994\n",
            "313/313 [==============================] - 8s 25ms/step - loss: 0.7194 - accuracy: 0.9833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7193984985351562, 0.983299970626831]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoIN0mexC_gc"
      },
      "source": [
        " # 19 Convolutional Neural Network- Part II (Urdu | Hindi)\r\n",
        " #in CNN data is unstructured, while ANN have structucred data, in cnn imagre recognation has 4 types asuch as classification\r\n",
        " #classification and localization, mean image divide into parts to image lozalioze, 3rd is object detecttion \r\n",
        " #object detection mean to define the object \r\n",
        " #instant segmenation its mean to drwa the bouindary line or segmnt around image\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA_yAGZ9EiBE"
      },
      "source": [
        "# 20 Convolutional Neural Network- Digital Image(Urdu | Hindi)\r\n",
        "#6 by 6 mean image have 6hight 6weidth if image gray chanl value1 while color chanl value 3\r\n",
        "# widdth have how many pixel height haow mant pixel and how many channel \r\n",
        "#6*6*3 it mean w=6 h=6 and RGB=3\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrgNAOEOFoi_"
      },
      "source": [
        "# 21 Convolutional Neural Network-Image Processing (Urdu | Hindi)\r\n",
        "#Topic # 22 Convolution Operation (Urdu | Hindi)\r\n",
        "#in CNN unstructructed data, \r\n",
        "#Features map, filter\r\n",
        "#filter mean define matrix such as 2by2 or 3by3 \r\n",
        "#Topic # 23 CNN Stride (Urdu | Hindi)\r\n",
        "#for example u have 4by4 image and it has 2by2filter , multiply filter and image, then move one pixel right or left\r\n",
        "#same u walk u decide brisk walk or fast same here filter decifde u move one pixel or two, this process is aclled stride and to achive this process we need to say this is filter size \r\n",
        "#image size and i is called to develop feature map\r\n",
        "#some time frpm large image size to small image size during this process few features loss this process is called padding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THSZ-WzLEghZ"
      },
      "source": [
        "#Topic # 24 CNN Padding (Urdu | Hindi)\r\n",
        "#example of shoes if not fit u puut something under it to fit it.\r\n",
        "#if u wave 3by3 image, u apply filter on it 3by3 now ur feature image is 2by2, in 2by2 image posibilty few information is miss, \r\n",
        "#from 2by2 when we convert 3by3 we apply pad add layer it is called padaing to maintain its valaue original 3by3  \r\n",
        "#by adding extra layer final image same orignal imagewithout any loss information "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcZL2utUFq4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "81b518a4-0ee9-4f3e-f424-b2cf6151ec7a"
      },
      "source": [
        "#Topic # 25CNN Padding (Urdu | Hindi) and ppoling\r\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Handson/PretrainedModel/chapter2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I30EnSCN7mAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7899e36-2233-432e-c95e-acc621e292ed"
      },
      "source": [
        "#Topic # 26 CNN Image Recognition- Exercise (Urdu | Hindi)\r\n",
        "#download datset\r\n",
        "import tensorflow.keras.datasets.mnist as mnist\r\n",
        "#load the minsit datset and save the result into\r\n",
        "(features_train, label_train), (features_test, label_test)= mnist.load_data()\r\n",
        "#print label contents\r\n",
        "#label_test\r\n",
        "#proint the label contents\r\n",
        "#label_train\r\n",
        "#find the shape of the data\r\n",
        "#features_test.shape\r\n",
        "#features_train.shape\r\n",
        "features_train = features_train.reshape(60000,28,28,1)\r\n",
        "#features_train.shape\r\n",
        "features_test = features_test.reshape(10000,28,28,1)\r\n",
        "#features_test.shape\r\n",
        "#normalize the data\r\n",
        "features_train = features_train / 255.0\r\n",
        "features_test = features_test /2550.\r\n",
        "#Add layers\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "np.random.seed(8)\r\n",
        "tf.random.set_seed(8)\r\n",
        "#create Model\r\n",
        "model = tf.keras.Sequential()\r\n",
        "#Add layer, CNN,AC function, Filter and stride\r\n",
        "conv_layer1 = layers.Conv2D(64,(3,3), activation='relu', input_shape=(28,28,1))\r\n",
        "conv_layer2 = layers.Conv2D(64,(3,3), activation='relu')\r\n",
        "#FC\r\n",
        "fc_layer1 = layers.Dense(128, activation='relu')\r\n",
        "fc_layer2 = layers.Dense(10, activation='softmax')\r\n",
        "#Max pooling\r\n",
        "model.add(conv_layer1)\r\n",
        "model.add(layers.MaxPooling2D(2,2))\r\n",
        "model.add(conv_layer2)\r\n",
        "model.add(layers.MaxPooling2D(2,2))\r\n",
        "model.add(fc_layer1)\r\n",
        "model.add(fc_layer2)\r\n",
        "##############\r\n",
        "#Add optimizer\r\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)\r\n",
        "#Compile Model\r\n",
        "model.compile(loss='sparse_categorical_crossentropy',\\\r\n",
        "           optimizer=optimizer, metrics=['accuracy'])\r\n",
        "#############\r\n",
        "#model summary\r\n",
        "model.summary()\r\n",
        "#validate model\r\n",
        "model.fit(features_train,label_train, epochs=5,\\\r\n",
        "          validation_split = 0.2, verbose=2)\r\n",
        "#model evulate\r\n",
        "model.evaluate(features_test, label_test)\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 5, 5, 128)         8320      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 5, 5, 10)          1290      \n",
            "=================================================================\n",
            "Total params: 47,178\n",
            "Trainable params: 47,178\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f42e1a76b5d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#validate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m#model evulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:1569 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4941 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:4241 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:4156 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (32,)) should equal the shape of logits except for the last dimension (received (800, 10)).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n3sTH99ekUG"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\r\n",
        "from tensorflow.keras import Model\r\n",
        "mnist = tf.keras.datasets.mnist\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\r\n",
        "\r\n",
        "# Add a channels dimension\r\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\r\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")\r\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\r\n",
        "    (x_train, y_train)).shuffle(10000).batch(32)\r\n",
        "\r\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\r\n",
        "class MyModel(Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(MyModel, self).__init__()\r\n",
        "    self.conv1 = Conv2D(32, 3, activation='relu')\r\n",
        "    self.flatten = Flatten()\r\n",
        "    self.d1 = Dense(128, activation='relu')\r\n",
        "    self.d2 = Dense(10)\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.flatten(x)\r\n",
        "    x = self.d1(x)\r\n",
        "    return self.d2(x)\r\n",
        "\r\n",
        "# Create an instance of the model\r\n",
        "model = MyModel()\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n",
        "\r\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywBo6r3KfFwg"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(images, labels):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    # training=True is only needed if there are layers with different\r\n",
        "    # behavior during training versus inference (e.g. Dropout).\r\n",
        "    predictions = model(images, training=True)\r\n",
        "    loss = loss_object(labels, predictions)\r\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "  train_loss(loss)\r\n",
        "  train_accuracy(labels, predictions)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF0UtZTwfHaT"
      },
      "source": [
        "@tf.function\r\n",
        "def test_step(images, labels):\r\n",
        "  # training=False is only needed if there are layers with different\r\n",
        "  # behavior during training versus inference (e.g. Dropout).\r\n",
        "  predictions = model(images, training=False)\r\n",
        "  t_loss = loss_object(labels, predictions)\r\n",
        "\r\n",
        "  test_loss(t_loss)\r\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7KVinSpfOgC",
        "outputId": "d75ac104-a112-4ca4-b3f1-82e51a79d261"
      },
      "source": [
        "EPOCHS = 5\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  # Reset the metrics at the start of the next epoch\r\n",
        "  train_loss.reset_states()\r\n",
        "  train_accuracy.reset_states()\r\n",
        "  test_loss.reset_states()\r\n",
        "  test_accuracy.reset_states()\r\n",
        "\r\n",
        "  for images, labels in train_ds:\r\n",
        "    train_step(images, labels)\r\n",
        "\r\n",
        "  for test_images, test_labels in test_ds:\r\n",
        "    test_step(test_images, test_labels)\r\n",
        "\r\n",
        "  print(\r\n",
        "    f'Epoch {epoch + 1}, '\r\n",
        "    f'Loss: {train_loss.result()}, '\r\n",
        "    f'Accuracy: {train_accuracy.result() * 100}, '\r\n",
        "    f'Test Loss: {test_loss.result()}, '\r\n",
        "    f'Test Accuracy: {test_accuracy.result() * 100}'\r\n",
        "  )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.13622990250587463, Accuracy: 95.91166687011719, Test Loss: 0.06594467163085938, Test Accuracy: 97.81999969482422\n",
            "Epoch 2, Loss: 0.04398464038968086, Accuracy: 98.60333251953125, Test Loss: 0.05772688239812851, Test Accuracy: 98.02999877929688\n",
            "Epoch 3, Loss: 0.02377179078757763, Accuracy: 99.22666931152344, Test Loss: 0.05195274576544762, Test Accuracy: 98.33999633789062\n",
            "Epoch 4, Loss: 0.015077217482030392, Accuracy: 99.51333618164062, Test Loss: 0.05054162070155144, Test Accuracy: 98.47999572753906\n",
            "Epoch 5, Loss: 0.010233435779809952, Accuracy: 99.67333221435547, Test Loss: 0.05757470056414604, Test Accuracy: 98.43000030517578\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}